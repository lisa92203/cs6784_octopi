{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_data(df, sensor_cols, window_size, step_size, variables_first, all_windows=False, label_list=None, constant_label=None):\n",
    "    \"\"\"\n",
    "    df - pandas dataframe with index corresponding to epoch time in milliseconds\n",
    "    sensor_cols - columns that should be included in the output sliding windows\n",
    "    window_size - number of samples (df rows) to use in each window\n",
    "    step_size - number of rows to step forward before creating the next window\n",
    "    variables_first - If True, return windows as a 3D array with shape (# samples, # variables, window_size)\n",
    "                      If False, return windows as a 3D array with shape (# samples, window_size, # variables)\n",
    "    all_windows - If False then only return windows (and labels and intervals) for labeled intervals (i.e., determined by \n",
    "                      label_list or constant_label). If True, then you will also get windows with a corresponding \n",
    "                      label of \"\" for unlabeled data and data that overlapped label boundaries\n",
    "    label_list - List of format [[StartTimestamp, EndTimestamp, LabelString, Subject], ] that will be used to assign labels \n",
    "                      to windows (e.g., used when loading from a file that has multiple intervals of activities with \n",
    "                      different classes)\n",
    "    constant_label - A single label string that should be applied to all windows (e.g., used when loading from a file \n",
    "                      that only contains a single class)\n",
    "    \"\"\"\n",
    "    assert step_size <= window_size\n",
    "    assert step_size > 0\n",
    "\n",
    "    max_gap = 100 #max gap allowed in sensor data in milliseconds\n",
    "\n",
    "    windowed_data = []\n",
    "    intervals = []\n",
    "    labels = []\n",
    "    subjects = [] \n",
    "\n",
    "    current_window = []\n",
    "    current_interval = []\n",
    "    current_labels = []\n",
    "    last_timestamp = None\n",
    "\n",
    "    if label_list is not None:\n",
    "        label_list.sort()\n",
    "        label_idx = 0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        row_values = [getattr(row, col) for col in sensor_cols ]\n",
    "        timestamp = getattr(row, \"Index\")\n",
    "\n",
    "        if last_timestamp is None:\n",
    "            last_timestamp = timestamp\n",
    "\n",
    "        if constant_label is not None:\n",
    "            label = constant_label\n",
    "            subject = None\n",
    "        elif label_list is not None:\n",
    "            label = \"\"\n",
    "            subject = None\n",
    "            for i in range(label_idx, len(label_list)):\n",
    "                if timestamp > label_list[i][1]:\n",
    "                    label_idx += 1\n",
    "                elif timestamp < label_list[i][0]:\n",
    "                    break\n",
    "                elif timestamp >= label_list[i][0] and timestamp <= label_list[i][1]:\n",
    "                    label = label_list[i][2]\n",
    "                    if len(label_list[i]) > 3:\n",
    "                        subject = label_list[i][3]\n",
    "                    break\n",
    "        else:\n",
    "            label = \"\"\n",
    "            subject = None\n",
    "\n",
    "        if not all_windows and len(current_labels) > 0 and label != current_labels[-1]:\n",
    "            current_window = []\n",
    "            current_interval = []\n",
    "            current_labels = []\n",
    "\n",
    "        if timestamp - last_timestamp > max_gap:\n",
    "            current_window = []\n",
    "            current_interval = []\n",
    "            current_labels = []\n",
    "        last_timestamp = timestamp\n",
    "\n",
    "        current_window.append(row_values)\n",
    "        current_interval.append(timestamp)\n",
    "        current_labels.append(label)\n",
    "\n",
    "        if len(current_window) == window_size:\n",
    "            if all_windows or label != \"\":\n",
    "                if variables_first:\n",
    "                    windowed_data.append(np.transpose(current_window))\n",
    "                else:\n",
    "                    windowed_data.append(np.array(current_window))\n",
    "\n",
    "                intervals.append((current_interval[0], current_interval[-1]))\n",
    "\n",
    "                if len(set(current_labels)) > 1:\n",
    "                    labels.append(\"\")\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "\n",
    "                subjects.append(subject)\n",
    "\n",
    "            for i in range(step_size):\n",
    "                current_window.pop(0)\n",
    "                current_interval.pop(0)\n",
    "                current_labels.pop(0)\n",
    "\n",
    "    return windowed_data, labels, intervals, subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_merging(primary_df, secondary_df_list):\n",
    "    \"\"\"\n",
    "    primary_df - this df will be used to decide the timestamps used for the combined_df\n",
    "    secondary_df_list - list of dfs that should be joined into primary_df.\n",
    "                        Any row with a timestamp that matches a timestamp in primary_df will have its values matched to that primary_df row\n",
    "                        Any rows in primary_df that don't have a match in the secondary df will have values backfilled to fill the gap.\n",
    "    \"\"\"\n",
    "    combined_df = primary_df.copy(deep=True)\n",
    "\n",
    "    for df2 in secondary_df_list:\n",
    "        combined_df = combined_df.join(df2, how=\"left\")\n",
    "    \n",
    "    combined_df.fillna(method='bfill', inplace=True)\n",
    "    combined_df.dropna(inplace=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file_list(csv_file_path_list, cols, prefix=None, sample_rate=None):\n",
    "    \"\"\"\n",
    "    csv_file_path_list - list of csv paths that should all be combined into a single df\n",
    "    cols - list of columns that should be used when creating the df\n",
    "    prefix - adds a prefix to sensor column names, useful when multiple sensors have a column with the same name (e.g., X)\n",
    "    sample_rate - Sample rate (in Hz) to use for the data. If None, it will be estimated from the data\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = None\n",
    "\n",
    "    for csv_file_path in csv_file_path_list:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file_path, usecols=cols)\n",
    "        except ValueError:\n",
    "            df = pd.read_csv(csv_file_path, names=cols)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"Empty csv file found: \" + str(csv_file_path), flush=True)\n",
    "            continue\n",
    "\n",
    "        if prefix is not None:\n",
    "            rename_cols = {c: prefix + \"_\" + c for c in cols if c != \"time\"}\n",
    "            df.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        if \".\" not in df.at[0, \"time\"]:\n",
    "            df.at[0, \"time\"] = df.at[0, \"time\"] + \".000\"\n",
    "\n",
    "        #https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#from-timestamps-to-epoch\n",
    "        stt_time = pd.to_datetime(df.at[0, \"time\"], format=\"%Y%m%d_%H:%M:%S.%f\").tz_localize('Asia/Tokyo').tz_convert(\"UTC\")\n",
    "        stt_time = (stt_time - pd.Timestamp(\"1970-01-01\", tz=\"UTC\")) / pd.Timedelta(\"1s\")\n",
    "        stt_time = round(stt_time, 2) #we're only dealing with up to 100 Hz, this should be updated if higher samping rates are used\n",
    "\n",
    "        if sample_rate is None:\n",
    "            if \".\" not in df.at[len(df[\"time\"])-1, \"time\"]:\n",
    "                df.at[len(df[\"time\"])-1, \"time\"] = df.at[len(df[\"time\"])-1, \"time\"] + \".000\"\n",
    "            end_time = pd.to_datetime(df.at[len(df[\"time\"])-1, \"time\"], format=\"%Y%m%d_%H:%M:%S.%f\").tz_localize('Asia/Tokyo').tz_convert(\"UTC\")\n",
    "            end_time = (end_time - pd.Timestamp(\"1970-01-01\", tz=\"UTC\")) / pd.Timedelta(\"1s\")\n",
    "            end_time = round(end_time, 2) #we're only dealing with up to 100 Hz, this should be updated if higher samping rates are used\n",
    "            sample_rate = round(len(df[\"time\"]) / (end_time - stt_time))\n",
    "\n",
    "        #set time values as milliseconds since epoch\n",
    "        time_values = np.array([ 1000 * (stt_time + (i/sample_rate)) for i in range(len(df[\"time\"])) ], dtype=np.int64)\n",
    "        df[\"time\"] = time_values\n",
    "\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    if combined_df is None:\n",
    "        return None\n",
    "    else:\n",
    "        combined_df.set_index(\"time\", inplace=True)\n",
    "        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_data_example(trimmed_dir, use_sensors):\n",
    "    print(\"trimmed cross scene example\")\n",
    "\n",
    "    trimmed_dir = os.path.join(trimmed_dir, \"sensors\")\n",
    "\n",
    "    sensor_dir_list = [\"acc_phone_clip\", \"acc_watch_clip\", \"gyro_clip\", \"orientation_clip\",]\n",
    "    subject_list = [\"subject\" + str(i) for i in range(1, 21)]\n",
    "    scene_list = [\"scene\" + str(i) for i in range(1, 5)]\n",
    "    session_list = [\"session\" + str(i) for i in range(1, 6)]\n",
    "\n",
    "    filename_case_map = {} #allows us to catch filenames that have capitalized letters\n",
    "\n",
    "    class_list = set()\n",
    "    for sensor_dir in sensor_dir_list:\n",
    "        for subject in subject_list:\n",
    "            for scene in scene_list:\n",
    "                for session in session_list:\n",
    "                    d = os.path.join(trimmed_dir, sensor_dir, subject, scene, session)\n",
    "                    if os.path.exists(d):\n",
    "                        for filename in os.listdir(d):\n",
    "                            if filename.endswith(\".csv\"):\n",
    "                                c = filename.split(\".\")[0].strip()\n",
    "                                c_lower = c.lower()\n",
    "                                if c_lower not in filename_case_map:\n",
    "                                    filename_case_map[c_lower] = [c_lower]\n",
    "                                if c not in filename_case_map[c_lower]:              \n",
    "                                    filename_case_map[c_lower].append(c)\n",
    "                                class_list.add(c_lower)\n",
    "    class_list = list(class_list)\n",
    "    class_list.sort()\n",
    "\n",
    "    train_scene_list = [\"scene1\", \"scene2\",\"scene3\"] #training scene from the challenge\n",
    "    test_scene_list = [\"scene4\"] #val scene from the challenge\n",
    "    for s in test_scene_list:\n",
    "        assert s not in train_scene_list\n",
    "    for s in train_scene_list:\n",
    "        assert s not in test_scene_list\n",
    "\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    train_intervals = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    test_intervals = []\n",
    "\n",
    "    for subject in subject_list:\n",
    "        print(subject, flush=True)\n",
    "        for scene in scene_list:\n",
    "            for session in session_list:\n",
    "                for label in class_list:\n",
    "                    acc_w_df = None\n",
    "                    acc_s_df = None\n",
    "                    gyro_df = None\n",
    "                    orientation_df = None                \n",
    "\n",
    "                    if \"acc_phone\" in use_sensors: \n",
    "                        for label_ in filename_case_map[label]:\n",
    "                            acc_s_path = os.path.join(trimmed_dir, \"acc_phone_clip\", subject, scene, session, label_ + \".csv\")\n",
    "                            if os.path.exists(acc_s_path):\n",
    "                                acc_s_df = load_csv_file_list(csv_file_path_list=[acc_s_path], cols=[\"time\", \"x\", \"y\", \"z\"], prefix=\"s_acc\", sample_rate=100)\n",
    "                        if acc_s_df is None:\n",
    "                            continue\n",
    "                        else:\n",
    "                            acc_s_df = acc_s_df / 9.8 #converting values from m/s^2 to g's\n",
    "\n",
    "                    if \"acc_watch\" in use_sensors: \n",
    "                        for label_ in filename_case_map[label]:\n",
    "                            acc_w_path = os.path.join(trimmed_dir, \"acc_watch_clip\", subject, scene, session, label_ + \".csv\")\n",
    "                            if os.path.exists(acc_w_path):\n",
    "                                acc_w_df = load_csv_file_list(csv_file_path_list=[acc_w_path], cols=[\"time\", \"x\", \"y\", \"z\"], prefix=\"w_acc\", sample_rate=100)\n",
    "                        if acc_w_df is None:\n",
    "                            continue\n",
    "                        else:\n",
    "                            acc_w_df = acc_w_df / 9.8 #converting values from m/s^2 to g's\n",
    "\n",
    "                    if \"gyro\" in use_sensors: \n",
    "                        for label_ in filename_case_map[label]:\n",
    "                            gyro_path = os.path.join(trimmed_dir, \"gyro_clip\", subject, scene, session, label_ + \".csv\")\n",
    "                            if os.path.exists(gyro_path):\n",
    "                                gyro_df = load_csv_file_list(csv_file_path_list=[gyro_path], cols=[\"time\", \"x\", \"y\", \"z\"], prefix=\"gyro\", sample_rate=50)\n",
    "                        if gyro_df is None:\n",
    "                            continue\n",
    "\n",
    "                    if \"orientation\" in use_sensors: \n",
    "                        for label_ in filename_case_map[label]:\n",
    "                            orientation_path = os.path.join(trimmed_dir, \"orientation_clip\", subject, scene, session, label_ + \".csv\")\n",
    "                            if os.path.exists(orientation_path):\n",
    "                                orientation_df = load_csv_file_list(csv_file_path_list=[orientation_path], cols=[\"time\", \"azimuth\", \"pitch\", \"roll\"], sample_rate=None)\n",
    "                        if orientation_df is None:\n",
    "                            continue\n",
    "\n",
    "                    if acc_s_df is not None:\n",
    "                        primary_df = acc_s_df\n",
    "                        secondary_df_list=[acc_w_df, gyro_df, orientation_df]\n",
    "                        secondary_df_list = [item for item in secondary_df_list if item is not None]\n",
    "\n",
    "                    elif acc_w_df is not None:\n",
    "                        primary_df = acc_w_df\n",
    "                        secondary_df_list=[acc_s_df, gyro_df, orientation_df]\n",
    "                        secondary_df_list = [item for item in secondary_df_list if item is not None]\n",
    "\n",
    "                    elif gyro_df is not None:\n",
    "                        primary_df = gyro_df\n",
    "                        secondary_df_list=[acc_s_df, acc_w_df, orientation_df]\n",
    "                        secondary_df_list = [item for item in secondary_df_list if item is not None]\n",
    "\n",
    "                    elif orientation_df is not None:\n",
    "                        primary_df = orientation_df\n",
    "                        secondary_df_list=[acc_s_df, acc_w_df, gyro_df]\n",
    "                        secondary_df_list = [item for item in secondary_df_list if item is not None]\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "\n",
    "                    combined_df = datetime_merging(primary_df=primary_df, secondary_df_list=secondary_df_list)\n",
    "\n",
    "                    all_windows = False\n",
    "                    variables_first = True\n",
    "                    sensor_cols = [col for col in combined_df.columns if col != \"time\"]\n",
    "                    window_size = 100 #1 second window of 100 Hz data\n",
    "                    step_size = 25 #75% overlap between windows\n",
    "\n",
    "                    X, y, intervals, _ = create_windowed_data(df=combined_df, sensor_cols=sensor_cols, window_size=window_size, step_size=step_size, variables_first=variables_first, all_windows=all_windows, label_list=None, constant_label=label.lower())\n",
    "\n",
    "                    if scene in train_scene_list:\n",
    "                        train_X.extend(X)\n",
    "                        train_y.extend(y)\n",
    "                        train_intervals.extend(intervals)\n",
    "\n",
    "                    elif scene in test_scene_list:\n",
    "                        test_X.extend(X)\n",
    "                        test_y.extend(y)\n",
    "                        test_intervals.extend(intervals)\n",
    "\n",
    "    train_val_split = int(0.9*len(train_X))\n",
    "    temp_train_X = train_X[:train_val_split]\n",
    "    temp_train_y = train_y[:train_val_split]\n",
    "    temp_train_intervals = train_intervals[:train_val_split]\n",
    "    val_X = train_X[train_val_split:]\n",
    "    val_y = train_y[train_val_split:]\n",
    "    val_intervals = train_intervals[train_val_split:]\n",
    "    train_X = temp_train_X\n",
    "    train_y = temp_train_y\n",
    "    train_intervals = temp_train_intervals\n",
    "\n",
    "    train_X = np.array(train_X)\n",
    "    train_y = np.array(train_y)\n",
    "    val_X = np.array(val_X)\n",
    "    val_y = np.array(val_y)\n",
    "    test_X = np.array(test_X)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(len(train_intervals))\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(len(val_intervals))\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "    print(len(test_intervals))\n",
    "\n",
    "    # X, y, splits = combine_split_data([train_X, val_X], [train_y, val_y])\n",
    "    # tfms  = [None, [Categorize()]]\n",
    "    # dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "    # dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[1024, 1024], shuffle=True, batch_tfms=[], num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed cross scene example\n",
      "subject1\n",
      "subject2\n",
      "Empty csv file found: /data3/cj/CS6784-Data/original_sensors/sensors/acc_phone_clip/subject2/scene4/session5/pushing.csv\n",
      "subject3\n",
      "subject4\n",
      "subject5\n",
      "subject6\n",
      "subject7\n",
      "subject8\n",
      "subject9\n",
      "subject10\n",
      "subject11\n",
      "subject12\n",
      "subject13\n",
      "subject14\n",
      "subject15\n",
      "subject16\n",
      "subject17\n",
      "subject18\n",
      "subject19\n",
      "subject20\n",
      "(139290, 6, 100)\n",
      "(139290,)\n",
      "139290\n",
      "(15477, 6, 100)\n",
      "(15477,)\n",
      "15477\n",
      "(66395, 6, 100)\n",
      "(66395,)\n",
      "66395\n"
     ]
    }
   ],
   "source": [
    "use_sensors = [\"acc_phone\", \"gyro\",] #[\"acc_phone\", \"acc_watch\", \"gyro\", \"orientation\",]\n",
    "\n",
    "trimmed_dir = \"/data3/cj/CS6784-Data/original_sensors\"\n",
    "\n",
    "trimmed_data_example(trimmed_dir=trimmed_dir, use_sensors=use_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
